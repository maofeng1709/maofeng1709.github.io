---
layout: post
title: "RL course by David Silver - Lecture 1: Introduction to Reinforcement Learning"
date: 2018-02-28
---

1. About RL
	intersection of many different fields of sciences:
	
	<img src="../image/RL_intersection.png", width="300px">
	
	the science of decision making: try to understand the optimal way to making decisions.
	
	relationships of different branches of ML:
	- no supervisor, just reward signal
	- feedback is delayed, not instantaneous
	- time really matters (sequential, non i.i.d data)
	- Agent’s actions affect the subsequent data it receives
	
	<img src="../image/Branches_ML.png", width="300px">
	
3. What the problem set is

	**Reinforcement learning is based on the reward hypothesis:** 
	> All goals can be described by the maximisation of expected
cumulative reward.

	A reward $R_t$ is a scalar feedback signal which indicates how well agent is doing at step $t$. The agent’s job is to maximise cumulative 
reward.

	- Goal: select actions to maximise total future reward
   - Actions may have long term consequences
	- Reward may be delayed
	- It may be better to sacrifice immediate reward to gain more
	- long-term reward

	<img src="../image/Agent_and_environment.jpeg" width="300px">
	
	The brain in the above image represents agent and the earth represents environment.

	**History**, the sequence of observations:
	$H_t = A_1, O_1, R_t, ... A_t, O_t, R_t $. What happens next depends on the history. e.g. how agent make next action and how environment select observation/reward
	
	**State**, the information used to determine what happens next: $S_t = f(H_t)$. Environment state is not visible or available, we use the agent state to build RL algorithms.
	
	An **information state** (a.k.a. **Markov state**) contains all useful information from the history. A State is Markov if and only if: $\mathbb{P}[S_{t+1} | S_t] = \mathbb{P}[S_{t+1}|S_1, ... ,S_t]$. The future is independent of the past given the present.
	
4. Solution methods
5. Key problems
