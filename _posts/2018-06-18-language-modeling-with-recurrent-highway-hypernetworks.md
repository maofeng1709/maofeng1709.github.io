---
layout: post
title: "Character-Level Language Modeling with Recurrent
Highway Hypernetworks -- [Joseph Suarez (2016)]"
date: 2018-06-18
categories: [readings, papers]
---

Paper reference: [Joseph Suarez (2016)](https://papers.nips.cc/paper/6919-language-modeling-with-recurrent-highway-hypernetworks.pdf)

### 1. Introduction

2 problems exist in current recurrent architectures:
- gradient flow: vanishing gradients could be alleviated by LSTM/GRU, batch normalization, RHNs ...
- weight-sharing: hypernetworks as a mechanism for allowing weight drift between timesteps

This paper introduce a new approach combining RHNs and hypernetworks to improve the recurrent architecture, and the give experimental/theoretical analysis on them.

### 2. Model architecture

#### 2.1 Recurrent highway networks

See another blog: [here](http://localhost:4000/readings/papers/2018-05-15/recurrent-highway-networks)

#### 2.2 Hypernetworks

The original paper is [here](https://arxiv.org/abs/1609.09106)

The idea of hypernetwork is to generate the weights of a large/main network by a small network. One of the advantages of hypernetworks is that it can realize the weight-drift in RNN, i.e. weights change in different time step.

Suppose $$a$$ is the activation output of an arbitrary recurrent architecture, the hypervector $$z$$ is defined as:

$$z(a) = W_p a$$ 

where $$W_p \in \mathbb{R}^{n \times h}$$ is a upscaling projection from dimension $$h$$ to $$n$$, and $$h \ll n$$. The hypervector is then used to scale the weights of the main recurrent network by:

$$\tilde{W}(z(a)) = 
\left(\begin{array}{cc} 
z_0(a)W_0\\
z_1(a)W_1\\
...\\
z_{n-1}(a)W_{n-1}\\
\end{array}\right)
$$

Here we apply a element-wise product across columns of $$W$$ and $$z$$, so we relax the weight sharing constraint implicit in RNNs.

Below is an example of basic RNN with adaptive weights generated by a hypernetwork:

![]({{site.url}}/assets/image/hypernetwork_rnn.png){:width="600px"}

The main network can be described by :

$$h_t = \phi(\tilde{W}_h(z_h)h_{t-1} + \tilde{W}_x(z_x)x_t + \tilde{b}(z_b))$$

The hypervector $$z_h$$ and $$z_x$$ are generated by the orange small recurrent network:

$$

\hat{x} = 
\left(\begin{array}{cc} 
h_{t-1}\\
x_t\\
\end{array}\right) \\
\hat{h}_t = \phi(W_{\hat{h}}\hat{h}_{t-1} + W_{\hat{x}}\hat{x}_t + \hat{b})\\
z_h = W_{\hat{h}h}\hat{h}_{t-1} + b_{\hat{h}h}\\
z_x = W_{\hat{h}x}\hat{h}_{t-1} + b_{\hat{h}x}\\
z_b = W_{\hat{h}b}\hat{h}_{t-1}

$$

#### 2.2 Recurrent highway hypernetworks

The recurrent highway hypernetwork is constructed by adapting  hypernetworks to RHNs by directly modifying the RHN cell.

Recall that in a RHN cell, the state transformation is done by a highway network of $$l$$ layers. The author uses a hypernetwork RHNCell () to generate $$s_h$$ for each layer, then $$s_h$$ serves as a seed to be upscaled to the hypervector $$z$$ and finally drift the weights in the main network RHNCellHyper().

Notice that the notation in the original paper is backward and maybe confusing: the hypernetwork is a standard, unmodified RHNCell; its outputs are then used in the main network, which is the modified RHNCellHyper.

### 3. Results, discussion and conclusion

In general, Recurrent highway hypernetworks get a better gradient flow. To be completed ...