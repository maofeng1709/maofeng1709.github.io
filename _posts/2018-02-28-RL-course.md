---
layout: post
title: "RL course by David Silver"
date: 2018-02-28
categories: [courses, reinforcement learning]
---

1. Overview
	
	![]({{site.url}}/assets/image/RL_intersection.png){:width="300px"}
	_RL as a intersection of multiple fields_
	
	the science of decision making: try to understand the optimal way to making decisions.
	
	relationships of different branches of ML:
	- no supervisor, just reward signal
	- feedback is delayed, not instantaneous
	- time really matters (sequential, non i.i.d data)
	- Agent’s actions affect the subsequent data it receives
	
	![]({{site.url}}/assets/image/Branches_ML.png){:width="300px"}
	_RL as a branch of machine learning_
	
2. What the problem set is

	A RL problem is a sequential decision making problem - controlling an agent to interact with environment step by step in order to achieve some goal.

	![]({{site.url}}/assets/image/Agent_and_environment.jpeg){:width="300px"}
	_Interaction between agent (brain) and environment (earth)_

	At each step $$t$$ the agent executes action $$A_t$$. The environment receives the action $$A_t$$ and emits observation $$O_t$$ and reward $$R_t$$, then the agent receives the observation and reward.
	**Reinforcement learning is based on the reward hypothesis:** 
	> All goals can be described by the maximisation of expected
cumulative reward.

	Let's go deeper into those elements:
	
	- A **reward** $$R_t$$ is a feedback signal which indicates how well agent is doing at step $$t$$. <br>The agent’s job is to select actions to maximise cumulative reward. In different pattern of agent/environment interaction, actions may have long term consequences and reward may be delayed, so that **it may be better to sacrifice immediate reward to gain more long-term reward**.
	
	- **History**, the sequence of observations: $$ H_t = A_1, O_1, R_t, ... A_t, O_t, R_t$$. <br>What happens next depends on the history. e.g. how agent make next action and how environment select observation/reward.
	
	- **State**, the information used to determine what happens next: $$S_t = f(H_t)$$. <br>Environment state is not visible or available, we use the agent state to build RL algorithms. An **information state** (a.k.a. **Markov state**) contains all useful information from the history. A State is Markov if and only if: 
$$\mathbb{P}[S_{t+1} | S_t] = \mathbb{P}[S_{t+1}|S_1, ... ,S_t]$$. That is to say, **the future is independent of the past given the present**.

	- **Observability**, whether the agent observes environment directly. <br>Full observability (Agent state = environment
state = information state) => Markov decision process (MDP), Partial observability => partially observable Markov decision process (POMDP).
	
3. Solution methods - how to solve the problem
	
	What is inside a RL agent? One or more of there components:
	- Policy: agent's behaviour function $$\pi$$. <br> deterministic policy: $$a = \pi(s)$$ V.S. stochastic policy:
$$\pi(a|s) = \mathbb{P}[A=a | S=s]$$
	- Value function: how good is each state and/or action. <br>More precisely, how much total reward will we get if following a particular policy.
	- Model: agent's representation of the environment. How to predict the next state and reward, which is optional in many RL algorithms. Actually many of them are model free and don't explicitly define a model at all. <br><br>

	![]({{site.url}}/assets/image/RL_agent_taxonomy.png){:width="300px"} _RL agent taxonomy_
	
	
4. Key problems

	Two fundamental problems in sequential decision making:

	- Reinforcement Learning: The environment is initially unknown.<br> The agent interacts with the environment and improves its policy.
	- Planning: The modal of environment is known. <br> The agent performs computations with the modal and find a good(optimal) policy.

	For the RL approach, it is like trial-and-error learning. The agent should discover a good policy during the experiences of interaction with environment with out losing too much reward along the way. Here the exploration and exploitation dilemma comes: the agent is supposed to find more information about the environment while exploits known information to maximise reward.
	

